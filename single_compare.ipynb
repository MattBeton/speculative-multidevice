{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51037286",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/frank/gpu_mode/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.cache_utils import DynamicCache, StaticCache\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from shared import (\n",
    "    MessageChannel,\n",
    "    PrefillRequest,\n",
    "    PrefillResponse,\n",
    "    PrefillBatchRequest,\n",
    "    PrefillBatchResponse,\n",
    "    ResetRequest,\n",
    "    VerifyRequest,\n",
    "    VerifyResponse,\n",
    "    VerifyBatchRequest,\n",
    "    VerifyBatchResponse,\n",
    "    VerifyResponseItem,\n",
    ")\n",
    "\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58f563ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = os.environ.get(\"HF_BASE_MODEL\", \"meta-llama/Llama-3.1-8B\")\n",
    "TOP_K = int(os.environ.get(\"HF_TOP_K\", \"20\"))\n",
    "ATTN_IMPL_ENV = os.environ.get(\"HF_ATTN_IMPL\", \"\").strip()  # e.g., \"flash_attention_2\" if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33b2136e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('mps')\n",
    "DTYPE = torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8eb1749a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.89s/it]\n"
     ]
    }
   ],
   "source": [
    "hf_token = os.environ.get(\"HF_TOKEN\", None)\n",
    "from_kwargs = {\n",
    "    \"dtype\": DTYPE,\n",
    "    \"device_map\": None,           # keep single process; move to one device below\n",
    "    \"low_cpu_mem_usage\": True,\n",
    "    \"token\": hf_token,\n",
    "    \"local_files_only\": True,\n",
    "}\n",
    "if ATTN_IMPL_ENV:\n",
    "    from_kwargs[\"attn_implementation\"] = ATTN_IMPL_ENV\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    **from_kwargs,\n",
    ").to(DEVICE) # type: ignore\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    BASE_MODEL, \n",
    "    use_fast=True, \n",
    "    token=hf_token, \n",
    "    local_files_only=True,\n",
    ")\n",
    "\n",
    "PAD_ID = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else (\n",
    "    tokenizer.eos_token_id if tokenizer.eos_token_id is not None else 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f070aed",
   "metadata": {},
   "source": [
    "## Prefill Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b133e3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "prompts_str = [\n",
    "    # \"def bubble_sort(x: list[int]):\",\n",
    "    \"What is the meaning of life\",\n",
    "]\n",
    "\n",
    "tokens: list[list[int]] = [tokenizer.encode(prompt) for prompt in prompts_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fc632c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[128000, 3923, 374, 279, 7438, 315, 2324]]\n"
     ]
    }
   ],
   "source": [
    "@torch.inference_mode()\n",
    "def prefill(model: nn.Module, tokens: list[list[int]]):\n",
    "    max_len = max(len(x) for x in tokens)\n",
    "    padded = [[PAD_ID] * (max_len - len(prompt)) + prompt for prompt in tokens]  # is pad id 0 correct?\n",
    "    x = torch.tensor(padded, dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    cache = DynamicCache(\n",
    "        config=model.config, \n",
    "    )\n",
    "\n",
    "    outputs = model(\n",
    "        input_ids=x, \n",
    "        past_key_values=cache, \n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    return outputs.past_key_values\n",
    "\n",
    "cache = prefill(model, tokens)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae4f7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[30]]\n",
      "tensor([7], device='mps:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:00<00:01, 17.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[128000, 3923, 374, 279, 7438, 315, 2324, 30]]\n",
      "[' This']\n",
      "[[128000, 3923, 374, 279, 7438, 315, 2324, 30, 1115]]\n",
      "[' is']\n",
      "[[128000, 3923, 374, 279, 7438, 315, 2324, 30, 1115, 374]]\n",
      "[' a']\n",
      "[[128000, 3923, 374, 279, 7438, 315, 2324, 30, 1115, 374, 264]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [00:00<00:00, 17.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' question']\n",
      "[[128000, 3923, 374, 279, 7438, 315, 2324, 30, 1115, 374, 264, 3488]]\n",
      "[' that']\n",
      "[[128000, 3923, 374, 279, 7438, 315, 2324, 30, 1115, 374, 264, 3488, 430]]\n",
      "[' has']\n",
      "[[128000, 3923, 374, 279, 7438, 315, 2324, 30, 1115, 374, 264, 3488, 430, 706]]\n",
      "[' puzzled']\n",
      "[[128000, 3923, 374, 279, 7438, 315, 2324, 30, 1115, 374, 264, 3488, 430, 706, 87420]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [00:00<00:00, 17.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' philosophers']\n",
      "[[128000, 3923, 374, 279, 7438, 315, 2324, 30, 1115, 374, 264, 3488, 430, 706, 87420, 61787]]\n",
      "[',']\n",
      "[[128000, 3923, 374, 279, 7438, 315, 2324, 30, 1115, 374, 264, 3488, 430, 706, 87420, 61787, 11]]\n",
      "[' theolog']\n",
      "[[128000, 3923, 374, 279, 7438, 315, 2324, 30, 1115, 374, 264, 3488, 430, 706, 87420, 61787, 11, 90602]]\n",
      "['ians']\n",
      "[[128000, 3923, 374, 279, 7438, 315, 2324, 30, 1115, 374, 264, 3488, 430, 706, 87420, 61787, 11, 90602, 5493]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [00:00<00:00, 17.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',']\n",
      "[[128000, 3923, 374, 279, 7438, 315, 2324, 30, 1115, 374, 264, 3488, 430, 706, 87420, 61787, 11, 90602, 5493, 11]]\n",
      "[' and']\n",
      "[[128000, 3923, 374, 279, 7438, 315, 2324, 30, 1115, 374, 264, 3488, 430, 706, 87420, 61787, 11, 90602, 5493, 11, 323]]\n",
      "[' scientists']\n",
      "[[128000, 3923, 374, 279, 7438, 315, 2324, 30, 1115, 374, 264, 3488, 430, 706, 87420, 61787, 11, 90602, 5493, 11, 323, 14248]]\n",
      "[' for']\n",
      "[[128000, 3923, 374, 279, 7438, 315, 2324, 30, 1115, 374, 264, 3488, 430, 706, 87420, 61787, 11, 90602, 5493, 11, 323, 14248, 369]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [00:01<00:00, 17.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' centuries']\n",
      "[[128000, 3923, 374, 279, 7438, 315, 2324, 30, 1115, 374, 264, 3488, 430, 706, 87420, 61787, 11, 90602, 5493, 11, 323, 14248, 369, 24552]]\n",
      "['.']\n",
      "[[128000, 3923, 374, 279, 7438, 315, 2324, 30, 1115, 374, 264, 3488, 430, 706, 87420, 61787, 11, 90602, 5493, 11, 323, 14248, 369, 24552, 13]]\n",
      "[' There']\n",
      "[[128000, 3923, 374, 279, 7438, 315, 2324, 30, 1115, 374, 264, 3488, 430, 706, 87420, 61787, 11, 90602, 5493, 11, 323, 14248, 369, 24552, 13, 2684]]\n",
      "[' are']\n",
      "[[128000, 3923, 374, 279, 7438, 315, 2324, 30, 1115, 374, 264, 3488, 430, 706, 87420, 61787, 11, 90602, 5493, 11, 323, 14248, 369, 24552, 13, 2684, 527]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 17.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' many']\n",
      "<|begin_of_text|>What is the meaning of life? This is a question that has puzzled philosophers, theologians, and scientists for centuries. There are\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     52\u001b[39m     \u001b[38;5;28mprint\u001b[39m([tokenizer.decode(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m generated])\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m4\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m     \u001b[38;5;28mprint\u001b[39m(tokenizer.decode(\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m))\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "@torch.inference_mode()\n",
    "def generate_step(\n",
    "    model: nn.Module,\n",
    "    cache: DynamicCache,\n",
    "    tokens: list[list[int]],\n",
    "    lengths: torch.LongTensor,\n",
    "):\n",
    "    x = torch.tensor(tokens, dtype=torch.long, device=DEVICE).view(-1, 1)\n",
    "\n",
    "    B = lengths.size(0)\n",
    "    S_prev = cache.layers[0].keys.shape[2]\n",
    "\n",
    "    # Use int64 (not bool) on MPS; build past + current token mask\n",
    "    attn_mask = torch.zeros((B, S_prev + 1), dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    starts = (S_prev - lengths).clamp_min(0)                # (B,)\n",
    "    idx = torch.arange(S_prev, device=DEVICE)[None, :]      # (1, S_prev)\n",
    "    attn_mask[:, :-1] = (idx >= starts[:, None]).to(torch.long)\n",
    "    attn_mask[:, -1] = 1\n",
    "\n",
    "    pos_ids = lengths.view(B, 1)  # new token's RoPE position (0..L_i)\n",
    "\n",
    "    out = model(\n",
    "        input_ids=x,\n",
    "        past_key_values=cache,\n",
    "        use_cache=True,\n",
    "        attention_mask=attn_mask,  # int mask\n",
    "        position_ids=pos_ids,\n",
    "    )\n",
    "\n",
    "    next_tok = out.logits[:, -1].argmax(dim=-1).tolist()\n",
    "    lengths = lengths + 1\n",
    "    return [[t] for t in next_tok], lengths\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "suffix_text = [\n",
    "    '?',\n",
    "]\n",
    "generated = [tokenizer.encode(x)[1:] for x in suffix_text]\n",
    "print(generated)\n",
    "\n",
    "lengths = torch.tensor([len(x) for x in tokens], dtype=torch.long, device=model.device)\n",
    "print(lengths)\n",
    "\n",
    "for _ in tqdm(range(20)):\n",
    "    tokens: list[list[int]] = [x + y for x, y in zip(tokens, generated)]\n",
    "    print(tokens)\n",
    "\n",
    "    generated, lengths = generate_step(model, cache, generated, lengths)\n",
    "    print([tokenizer.decode(x) for x in generated])\n",
    "\n",
    "print(tokenizer.decode(tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29dc322",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
