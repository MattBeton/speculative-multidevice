{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "925c36aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/frank/gpu_mode/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.cache_utils import DynamicCache, StaticCache\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from shared import (\n",
    "    MessageChannel,\n",
    "    PrefillRequest,\n",
    "    PrefillResponse,\n",
    "    PrefillBatchRequest,\n",
    "    PrefillBatchResponse,\n",
    "    ResetRequest,\n",
    "    VerifyRequest,\n",
    "    VerifyResponse,\n",
    "    VerifyBatchRequest,\n",
    "    VerifyBatchResponse,\n",
    "    VerifyResponseItem,\n",
    ")\n",
    "\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87a42e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE_MODEL = os.environ.get(\"HF_BASE_MODEL\", \"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "# BASE_MODEL = os.environ.get(\"HF_BASE_MODEL\", \"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "BASE_MODEL = os.environ.get(\"HF_BASE_MODEL\", \"meta-llama/Llama-3.1-8B\")\n",
    "TOP_K = int(os.environ.get(\"HF_TOP_K\", \"20\"))\n",
    "ATTN_IMPL_ENV = os.environ.get(\"HF_ATTN_IMPL\", \"\").strip()  # e.g., \"flash_attention_2\" if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72c8c50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('mps')\n",
    "DTYPE = torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "998d5a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.05it/s]\n"
     ]
    }
   ],
   "source": [
    "hf_token = os.environ.get(\"HF_TOKEN\", None)\n",
    "from_kwargs = {\n",
    "    \"dtype\": DTYPE,\n",
    "    \"device_map\": None,           # keep single process; move to one device below\n",
    "    \"low_cpu_mem_usage\": True,\n",
    "    \"token\": hf_token,\n",
    "    \"local_files_only\": True,\n",
    "}\n",
    "if ATTN_IMPL_ENV:\n",
    "    from_kwargs[\"attn_implementation\"] = ATTN_IMPL_ENV\n",
    "\n",
    "import os\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    **from_kwargs,\n",
    ").to(DEVICE) # type: ignore\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    BASE_MODEL, \n",
    "    use_fast=True, \n",
    "    token=hf_token, \n",
    "    local_files_only=True,\n",
    ")\n",
    "\n",
    "PAD_ID = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else (\n",
    "    tokenizer.eos_token_id if tokenizer.eos_token_id is not None else 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4478cdd6",
   "metadata": {},
   "source": [
    "## Prefill Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c7cac08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "prompts_str = [\n",
    "    \"Explanation of speculative decoding in simple terms\",\n",
    "    \"This is a terse haiku about Apple MLX\",\n",
    "    \"def bubble_sort(x: list[int])\",\n",
    "    \"Why is the sky blue\",\n",
    "]\n",
    "\n",
    "tokens: list[list[int]] = [tokenizer.encode(prompt) for prompt in prompts_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b728b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[128000, 70869, 315, 66836, 48216, 304, 4382, 3878], [128000, 2028, 374, 264, 51637, 6520, 39342, 922, 8325, 20187, 55], [128000, 755, 24529, 18942, 2120, 25, 1160, 19155, 2526], [128000, 10445, 374, 279, 13180, 6437]]\n"
     ]
    }
   ],
   "source": [
    "# def mask_and_pos_ids(L: list[int]): # TODO: Should this be a list or a tensor?\n",
    "#     max_len = max(L)\n",
    "\n",
    "#     attention_mask = torch.zeros((len(L), max_len), dtype=torch.long, device=DEVICE)\n",
    "#     for i, l in enumerate(L):\n",
    "#         attention_mask[i, max_len - l:] = 1\n",
    "\n",
    "#     # position_ids / cache_position: 0..L_i-1 for non-pad tokens, 0 for pads\n",
    "#     # This works for both absolute and RoPE-style position handling.\n",
    "#     position_ids = (attention_mask.cumsum(dim=-1) - 1).clamp_min(0)\n",
    "#     position_ids = position_ids.masked_fill(attention_mask == 0, 0)\n",
    "\n",
    "#     return attention_mask, position_ids \n",
    "\n",
    "def mask_and_pos_ids(L: list[int]): # TODO: Should this be a list or a tensor?\n",
    "    max_len = max(L)\n",
    "\n",
    "    attention_mask = torch.zeros((len(L), max_len), dtype=torch.long, device=DEVICE)\n",
    "    for i, l in enumerate(L):\n",
    "        attention_mask[i, max_len - l:] = 1\n",
    "\n",
    "    # position_ids / cache_position: 0..L_i-1 for non-pad tokens, 0 for pads\n",
    "    # This works for both absolute and RoPE-style position handling.\n",
    "    position_ids = (attention_mask.cumsum(dim=-1) - 1).clamp_min(0)\n",
    "    position_ids = position_ids.masked_fill(attention_mask == 0, 0)\n",
    "\n",
    "    return attention_mask, position_ids \n",
    "\n",
    "@torch.inference_mode()\n",
    "def prefill(model: nn.Module, tokens: list[list[int]]):\n",
    "    max_len = max(len(x) for x in tokens)\n",
    "    padded = [[PAD_ID] * (max_len - len(prompt)) + prompt for prompt in tokens]  # is pad id 0 correct?\n",
    "    x = torch.tensor(padded, dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    cache = DynamicCache(\n",
    "        config=model.config, \n",
    "    )\n",
    "\n",
    "    attention_mask, position_ids = mask_and_pos_ids([len(x) for x in tokens])\n",
    "\n",
    "    outputs = model(\n",
    "        input_ids=x, \n",
    "        # attention_mask=attention_mask,\n",
    "        past_key_values=cache, \n",
    "        use_cache=True,\n",
    "        # position_ids=position_ids,\n",
    "    )\n",
    "\n",
    "    return outputs.past_key_values\n",
    "\n",
    "cache = prefill(model, tokens)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec609e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_cache(cache: DynamicCache, lengths: list[int]):\n",
    "    assert cache.layers[0].keys is not None and cache.layers[0].values is not None\n",
    "    B = cache.layers[0].keys.shape[0]\n",
    "\n",
    "    # Prepare destination cache\n",
    "    dst = DynamicCache()\n",
    "\n",
    "    for layer in range(len(cache)):\n",
    "        K = cache.layers[layer].keys\n",
    "        V = cache.layers[layer].values\n",
    "        assert K is not None and V is not None\n",
    "\n",
    "        _, H, S, D = K.shape\n",
    "\n",
    "        K_new = K.new_zeros((B, H, S, D))\n",
    "        V_new = V.new_zeros((B, H, S, D))\n",
    "\n",
    "        # Copy per row\n",
    "        for i in range(B):\n",
    "            length = lengths[i]\n",
    "            if length == 0:\n",
    "                continue\n",
    "            # surviving tokens are the first 'keep' positions (earliest..latest-rollback)\n",
    "            K_src = K[i, :, S-length:, :]\n",
    "            V_src = V[i, :, S-length:, :]\n",
    "\n",
    "            # right-aligned → write to the right, pad on the left implicitly\n",
    "            K_new[i, :, S-length:, :] = K_src\n",
    "            V_new[i, :, S-length:, :] = V_src\n",
    "\n",
    "        # print(dst.layers[layer].keys[i, 0, :, 0])\n",
    "        dst.update(K_new, V_new, layer)\n",
    "\n",
    "    return dst\n",
    "\n",
    "cache = zero_cache(cache, [len(x) for x in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb2e9768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0000, -3.5273,  0.1252,  0.0000], device='mps:0',\n",
      "       dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "print(cache.layers[0].keys[:, 0, 2, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a146fac4",
   "metadata": {},
   "source": [
    "## Pure Decode (Just A Sanity Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a41a74b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def generate_step(\n",
    "    model: nn.Module,\n",
    "    cache: DynamicCache,\n",
    "    tokens: list[list[int]],\n",
    "    lengths: torch.LongTensor,\n",
    "):\n",
    "    x = torch.tensor(tokens, dtype=torch.long, device=DEVICE).view(-1, 1)\n",
    "\n",
    "    B = lengths.size(0)\n",
    "    S_prev = cache.layers[0].keys.shape[2]\n",
    "\n",
    "    # Use int64 (not bool) on MPS; build past + current token mask\n",
    "    attn_mask = torch.zeros((B, S_prev + 1), dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    starts = (S_prev - lengths).clamp_min(0)                # (B,)\n",
    "    idx = torch.arange(S_prev, device=DEVICE)[None, :]      # (1, S_prev)\n",
    "    attn_mask[:, :-1] = (idx >= starts[:, None]).to(torch.long)\n",
    "    attn_mask[:, -1] = 1\n",
    "\n",
    "    pos_ids = lengths.view(B, 1)  # new token's RoPE position (0..L_i)\n",
    "\n",
    "    out = model(\n",
    "        input_ids=x,\n",
    "        past_key_values=cache,\n",
    "        use_cache=True,\n",
    "        attention_mask=attn_mask,  # int mask\n",
    "        position_ids=pos_ids,\n",
    "    )\n",
    "\n",
    "    next_tok = out.logits[:, -1].argmax(dim=-1).tolist()\n",
    "    lengths = lengths + 1\n",
    "    return [[t] for t in next_tok], lengths\n",
    "\n",
    "\n",
    "\n",
    "# suffix_text = ':'\n",
    "# generated = [[x] for x in tokenizer.encode(suffix_text)[1:]] * 4\n",
    "\n",
    "# lengths = torch.tensor([len(x) for x in tokens], dtype=torch.long, device=model.device)\n",
    "# print(lengths)\n",
    "\n",
    "# for _ in tqdm(range(20)):\n",
    "#     tokens: list[list[int]] = [x + y for x, y in zip(tokens, generated)]\n",
    "#     # print(tokens)\n",
    "\n",
    "#     generated, lengths = generate_step(model, cache, generated, lengths)\n",
    "#     # print(generated)\n",
    "\n",
    "# for i in range(4):\n",
    "#     print(tokenizer.decode(tokens[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58562114",
   "metadata": {},
   "source": [
    "## Experimental Verify Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0fea74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 11, 128])\n",
      "torch.Size([4, 8, 15, 128])\n"
     ]
    }
   ],
   "source": [
    "@torch.inference_mode()\n",
    "def verify(model: nn.Module, cache: DynamicCache, tokens: list[list[int]], draft_logits: np.array):\n",
    "    assert all([len(x) == len(tokens[0]) for x in tokens])\n",
    "    x = torch.tensor(tokens, dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    print(cache.layers[0].keys.shape)\n",
    "\n",
    "    outputs = model(\n",
    "        x, \n",
    "        use_cache=True, \n",
    "        past_key_values=cache\n",
    "    )\n",
    "\n",
    "    print(cache.layers[0].keys.shape)\n",
    "\n",
    "\n",
    "suffix_text = ': a short story'\n",
    "suffix_tokens = [tokenizer.encode(suffix_text)[1:] for _ in range(4)]\n",
    "\n",
    "tokens = [x + y for x, y in zip(tokens, suffix_tokens)]\n",
    "\n",
    "verify(model, cache, suffix_tokens, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33875371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|begin_of_text|>Explanation of speculative decoding in simple terms: a short story', '<|begin_of_text|>This is a terse haiku about Apple MLX: a short story', '<|begin_of_text|>def bubble_sort(x: list[int]): a short story', '<|begin_of_text|>Why is the sky blue: a short story']\n",
      "tensor([ 0.0000,  0.0000,  0.1252, -6.6953, -2.9492,  5.2969,  7.7773,  2.5840,\n",
      "        -2.9531, -6.8164, -2.0684,  0.4590,  5.4766,  4.9961, -1.1309],\n",
      "       device='mps:0', dtype=torch.float16)\n",
      "['<|begin_of_text|>Explanation of speculative decoding in simple terms:', '<|begin_of_text|>This is a terse haiku about Apple MLX: a short story', '<|begin_of_text|>def bubble_sort(x: list[int]):', '<|begin_of_text|>Why']\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.1252, -6.6953, -2.9492,\n",
      "         5.2969,  7.7773,  2.5840, -2.9531, -6.8164, -2.0684,  0.4590],\n",
      "       device='mps:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "def rollback_dynamic_per_row_simple(cache: DynamicCache, tokens: list[list[int]], r: list[int]):\n",
    "    \"\"\"\n",
    "    Roll back r[i] tokens for each batch row i in a DynamicCache.\n",
    "    The output cache maintains the same sequence length as the input, padding with zeros where needed.\n",
    "    \"\"\"\n",
    "    assert cache.layers[0].keys is not None and cache.layers[0].values is not None\n",
    "    assert all([x >= 0 for x in r])\n",
    "    B = cache.layers[0].keys.shape[0]\n",
    "    device = cache.layers[0].keys.device\n",
    "    dtype = cache.layers[0].keys.dtype\n",
    "\n",
    "    # Prepare destination cache\n",
    "    dst = DynamicCache()\n",
    "\n",
    "    for layer in range(len(cache)):\n",
    "        K = cache.layers[layer].keys\n",
    "        V = cache.layers[layer].values\n",
    "        assert K is not None and V is not None\n",
    "\n",
    "        _, H, S, D = K.shape\n",
    "\n",
    "        K_new = K.new_zeros((B, H, S, D))\n",
    "        V_new = V.new_zeros((B, H, S, D))\n",
    "\n",
    "        # Copy per row\n",
    "        for i in range(B):\n",
    "            keep = S - r[i]\n",
    "            if keep <= 0:\n",
    "                continue\n",
    "            # surviving tokens are the first 'keep' positions (earliest..latest-rollback)\n",
    "            K_src = K[i, :, :keep, :]\n",
    "            V_src = V[i, :, :keep, :]\n",
    "\n",
    "            # right-aligned → write to the right, pad on the left implicitly\n",
    "            start = S - keep\n",
    "            K_new[i, :, start:, :] = K_src\n",
    "            V_new[i, :, start:, :] = V_src\n",
    "            # print(K_new[i, 0, :, 0])\n",
    "\n",
    "        # print(dst.layers[layer].keys[i, 0, :, 0])\n",
    "        dst.update(K_new, V_new, layer)\n",
    "\n",
    "    tokens = [x[:len(x) - trim] for x, trim in zip(tokens, r)]\n",
    "\n",
    "    return dst, tokens\n",
    "\n",
    "rollback_values = data=[3, 0, 3, 8]\n",
    "print([tokenizer.decode(x) for x in tokens])\n",
    "print(cache.layers[0].keys[2, 0, :, 0])\n",
    "new_cache, tokens = rollback_dynamic_per_row_simple(cache, tokens, rollback_values)\n",
    "print([tokenizer.decode(x) for x in tokens])\n",
    "print(new_cache.layers[0].keys[2, 0, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f581e8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Explanation of speculative decoding in simple terms:\n",
      "<|begin_of_text|>This is a terse haiku about Apple MLX: a short story\n",
      "<|begin_of_text|>def bubble_sort(x: list[int]):\n",
      "<|begin_of_text|>Why\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(tokenizer.decode(tokens[i]))\n",
    "\n",
    "# raise Exception('stop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a585ea50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[66836], [8325], [720], [374]]\n",
      "tensor([ 9, 15, 10,  2], device='mps:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:00<00:04,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' decoding', ' has', ':', '://']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:00<00:03,  4.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' in', ' been', ' a', ' story']\n",
      "[' simple', ' telling', ' a', '\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [00:01<00:03,  4.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' decoding', ' about', ' a', 'The']\n",
      "[' in', ' its', ' a', ' story']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [00:01<00:02,  5.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' simple', ' machine', ' a', ' is']\n",
      "[' decoding', ' learning', ' a', ' about']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [00:01<00:02,  5.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[':', ' technology', ' a', ' a']\n",
      "[' a', '.', ' a', ' young']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [00:02<00:01,  5.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' short', ' The', ' a', ' man']\n",
      "[' story', ' story', ' a', ' named']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [00:02<00:01,  5.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[':', ' is', ' a', ' Jack']\n",
      "[' a', ' about', ' a', ' who']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [00:03<00:00,  5.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' short', ' how', ' a', ' is']\n",
      "[' story', ' Apple', ' a', ' a']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [00:03<00:00,  5.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[':', '’s', ' a', ' passionate']\n",
      "[' a', ' machine', ' a', ' writer']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [00:03<00:00,  4.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' simple', ' learning', ' a', ' but']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:04<00:00,  4.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' decoding', ' technology', ' a', ' struggling']\n",
      "[':', ' is', ' a', ' to']\n",
      "<|begin_of_text|>Explanation of speculative decoding in simple terms: speculative decoding in simple decoding in simple decoding: a short story: a short story: a simple decoding\n",
      "<|begin_of_text|>This is a terse haiku about Apple MLX: a short story Apple has been telling about its machine learning technology. The story is about how Apple’s machine learning technology\n",
      "<|begin_of_text|>def bubble_sort(x: list[int]): \n",
      ": a a a a a a a a a a a a a a a a a a\n",
      "<|begin_of_text|>Why is:// story\n",
      "The story is about a young man named Jack who is a passionate writer but struggling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "suffix_text = [\n",
    "    ' speculative',\n",
    "    ' Apple',\n",
    "    ' \\n',\n",
    "    ' is',\n",
    "]\n",
    "# generated = [[x] for x in tokenizer.encode(suffix_text)[1:]] * 4\n",
    "generated = [tokenizer.encode(x)[1:] for x in suffix_text]\n",
    "print(generated)\n",
    "\n",
    "lengths = torch.tensor([len(x) for x in tokens], dtype=torch.long, device=model.device)\n",
    "print(lengths)\n",
    "\n",
    "for _ in tqdm(range(20)):\n",
    "    tokens: list[list[int]] = [x + y for x, y in zip(tokens, generated)]\n",
    "    # print(tokens)\n",
    "\n",
    "    generated, lengths = generate_step(model, cache, generated, lengths)\n",
    "    print([tokenizer.decode(x) for x in generated])\n",
    "\n",
    "for i in range(4):\n",
    "    print(tokenizer.decode(tokens[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1b10fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([29, 35, 30, 22], device='mps:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "149aeba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suffix_text = '.'\n",
    "# generated = [[x] for x in tokenizer.encode(suffix_text)[1:]] * 3\n",
    "\n",
    "# lengths = torch.LongTensor([len(x) for x in tokens], device=model.device)\n",
    "\n",
    "# for _ in tqdm(range(20)):\n",
    "    \n",
    "\n",
    "#     generated, lengths = generate_step(model, cache, tokens, lengths)\n",
    "#     tokens: list[list[int]] = [x + y for x, y in zip(full_tokens, tokens)]\n",
    "\n",
    "# for i in range(3):\n",
    "#     print(tokenizer.decode(full_tokens[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e7129e",
   "metadata": {},
   "source": [
    "# Bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfda1995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rollback_dynamic_per_row(cache: DynamicCache, r: torch.LongTensor):\n",
    "#     \"\"\"\n",
    "#     Roll back r[i] tokens for each batch row i in a DynamicCache.\n",
    "#     Returns a *new* DynamicCache with time dim equal to max(L_i - r_i).\n",
    "#     \"\"\"\n",
    "#     assert cache.layers[0].keys is not None\n",
    "#     B = cache.layers[0].keys.shape[0]\n",
    "#     device = cache.layers[0].keys.device\n",
    "#     uniq = torch.unique(r)\n",
    "\n",
    "#     # Build empty destination (we'll fill layer by layer)\n",
    "#     dst = DynamicCache()\n",
    "#     for layer in range(len(cache)):\n",
    "#         K = cache.layers[layer].keys\n",
    "#         V = cache.layers[layer].keys\n",
    "#         assert K is not None\n",
    "#         assert V is not None\n",
    "\n",
    "#         B, H, S_old, D = K.shape\n",
    "#         # Compute new per-row lengths after rollback\n",
    "#         L_after = torch.full((B,), S_old, dtype=torch.long, device=device) - r\n",
    "#         S_new = int(L_after.max().item())\n",
    "\n",
    "#         K_new = K.new_zeros(B, H, S_new, D)\n",
    "#         V_new = V.new_zeros(B, H, S_new, D)\n",
    "\n",
    "#         # For each rollback bucket, crop and scatter back\n",
    "#         for rv in uniq.tolist():\n",
    "#             idx = (r == rv).nonzero(as_tuple=False).squeeze(-1)\n",
    "#             if idx.numel() == 0: \n",
    "#                 continue\n",
    "#             # Select sub-batch, crop rv tokens from the right\n",
    "#             K_sub = K.index_select(0, idx)\n",
    "#             V_sub = V.index_select(0, idx)\n",
    "\n",
    "#             # physical crop for this bucket\n",
    "#             S_keep = S_old - rv\n",
    "#             K_sub = K_sub[..., :S_keep, :]\n",
    "#             V_sub = V_sub[..., :S_keep, :]\n",
    "\n",
    "#             # place back into dst; zero-filling beyond S_keep keeps them \"rolled back\"\n",
    "#             K_new.index_copy_(0, idx, torch.nn.functional.pad(K_sub, (0,0,0,0,0, S_new - S_keep)))\n",
    "#             V_new.index_copy_(0, idx, torch.nn.functional.pad(V_sub, (0,0,0,0,0, S_new - S_keep)))\n",
    "\n",
    "#         dst.update(K_new, V_new, layer)\n",
    "\n",
    "#     return dst\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
