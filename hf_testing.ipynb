{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd5bc34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adb85f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/frank/gpu_mode/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.cache_utils import DynamicCache, StaticCache\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from shared import (\n",
    "    MessageChannel,\n",
    "    PrefillRequest,\n",
    "    PrefillResponse,\n",
    "    PrefillBatchRequest,\n",
    "    PrefillBatchResponse,\n",
    "    ResetRequest,\n",
    "    VerifyRequest,\n",
    "    VerifyResponse,\n",
    "    VerifyBatchRequest,\n",
    "    VerifyBatchResponse,\n",
    "    VerifyResponseItem,\n",
    ")\n",
    "\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from const import DEVICE, BASE_MODEL\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "561d0aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08066180",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.02it/s]"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model(BASE_MODEL)\n",
    "\n",
    "PAD_ID = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else (\n",
    "    tokenizer.eos_token_id if tokenizer.eos_token_id is not None else 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4478cdd6",
   "metadata": {},
   "source": [
    "## Prefill Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7cac08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "prompts_str = [\n",
    "    \"Explanation of speculative decoding in simple terms\",\n",
    "    \"This is a terse haiku about Apple MLX\",\n",
    "    \"def bubble_sort(x: list[int])\",\n",
    "    \"Why is the sky blue\",\n",
    "]\n",
    "\n",
    "tokens: list[list[int]] = [tokenizer.encode(prompt) for prompt in prompts_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b728b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], device='mps:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "cache = prefill(model, tokens)\n",
    "\n",
    "print_cache(cache, 2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec609e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def zero_cache(cache: DynamicCache, lengths: list[int]):\n",
    "#     assert cache.layers[0].keys is not None and cache.layers[0].values is not None\n",
    "#     B = cache.layers[0].keys.shape[0]\n",
    "\n",
    "#     # Prepare destination cache\n",
    "#     dst = DynamicCache()\n",
    "\n",
    "#     for layer in range(len(cache)):\n",
    "#         K = cache.layers[layer].keys\n",
    "#         V = cache.layers[layer].values\n",
    "#         assert K is not None and V is not None\n",
    "\n",
    "#         _, H, S, D = K.shape\n",
    "\n",
    "#         K_new = K.new_zeros((B, H, S, D))\n",
    "#         V_new = V.new_zeros((B, H, S, D))\n",
    "\n",
    "#         # Copy per row\n",
    "#         for i in range(B):\n",
    "#             length = lengths[i]\n",
    "#             if length == 0:\n",
    "#                 continue\n",
    "#             # surviving tokens are the first 'keep' positions (earliest..latest-rollback)\n",
    "#             K_src = K[i, :, S-length:, :]\n",
    "#             V_src = V[i, :, S-length:, :]\n",
    "\n",
    "#             # right-aligned → write to the right, pad on the left implicitly\n",
    "#             K_new[i, :, S-length:, :] = K_src\n",
    "#             V_new[i, :, S-length:, :] = V_src\n",
    "\n",
    "#         # print(dst.layers[layer].keys[i, 0, :, 0])\n",
    "#         dst.update(K_new, V_new, layer)\n",
    "\n",
    "#     return dst\n",
    "\n",
    "# cache = zero_cache(cache, [len(x) for x in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2e9768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3.1094,  3.1094,  0.4473,  0.9531, -5.1562, -8.1250, -2.8125,  1.5000,\n",
      "         6.7812,  3.0156, -1.3984], device='mps:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "print_cache(cache, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a146fac4",
   "metadata": {},
   "source": [
    "## Pure Decode (Just A Sanity Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41a74b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suffix_text = ':'\n",
    "# generated = [[x] for x in tokenizer.encode(suffix_text)[1:]] * 4\n",
    "\n",
    "# lengths = torch.tensor([len(x) for x in tokens], dtype=torch.long, device=model.device)\n",
    "# print(lengths)\n",
    "\n",
    "# for _ in tqdm(range(20)):\n",
    "#     tokens: list[list[int]] = [x + y for x, y in zip(tokens, generated)]\n",
    "#     # print(tokens)\n",
    "\n",
    "#     generated, lengths = generate_step(model, cache, generated, lengths)\n",
    "#     # print(generated)\n",
    "\n",
    "# for i in range(4):\n",
    "#     print(tokenizer.decode(tokens[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58562114",
   "metadata": {},
   "source": [
    "## Experimental Verify Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fea74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @torch.inference_mode()\n",
    "# def verify(model: nn.Module, cache: DynamicCache, tokens: list[list[int]], draft_logits: np.array):\n",
    "#     assert all([len(x) == len(tokens[0]) for x in tokens])\n",
    "#     x = torch.tensor(tokens, dtype=torch.long, device=DEVICE)\n",
    "\n",
    "#     print(cache.layers[0].keys.shape)\n",
    "\n",
    "#     outputs = model(\n",
    "#         x, \n",
    "#         use_cache=True, \n",
    "#         past_key_values=cache\n",
    "#     )\n",
    "\n",
    "#     print(cache.layers[0].keys.shape)\n",
    "\n",
    "\n",
    "# suffix_text = ': a short story'\n",
    "# suffix_tokens = [tokenizer.encode(suffix_text)[1:] for _ in range(4)]\n",
    "\n",
    "# tokens = [x + y for x, y in zip(tokens, suffix_tokens)]\n",
    "\n",
    "# verify(model, cache, suffix_tokens, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33875371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|begin_of_text|>Explanation of speculative decoding in simple terms', '<|begin_of_text|>This is a terse haiku about Apple MLX', '<|begin_of_text|>def bubble_sort(x: list[int])', '<|begin_of_text|>Why is the sky blue']\n",
      "tensor([ 3.1094,  3.1094,  0.4473,  0.9531, -5.1562, -8.1250, -2.8125,  1.5000,\n",
      "         6.7812,  3.0156, -1.3984], device='mps:0', dtype=torch.bfloat16)\n",
      "['<|begin_of_text|>Explanation of speculative decoding', '<|begin_of_text|>This is a terse haiku about Apple MLX', '<|begin_of_text|>def bubble_sort(x:', '<|begin_of_text|>Why is the']\n",
      "tensor([ 3.1094,  3.1094,  0.4473,  0.9531, -5.1562, -8.1250, -2.8125,  1.5000,\n",
      "         6.7812,  3.0156, -1.3984], device='mps:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "rollback_values = data=[3, 0, 3, 8]\n",
    "print([tokenizer.decode(x) for x in tokens])\n",
    "print_cache(cache, 2)\n",
    "new_cache, tokens = rollback_dynamic_per_row_simple(cache, tokens, rollback_values)\n",
    "print([tokenizer.decode(x) for x in tokens])\n",
    "print_cache(cache, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed6b7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_layer(cache: DynamicCache, b: int):\n",
    "#     \"\"\"\n",
    "#     Roll back r[i] tokens for each batch row i in a DynamicCache.\n",
    "#     The output cache maintains the same sequence length as the input, padding with zeros where needed.\n",
    "#     \"\"\"\n",
    "#     assert cache.layers[0].keys is not None and cache.layers[0].values is not None\n",
    "#     B = cache.layers[0].keys.shape[0]\n",
    "\n",
    "#     # Prepare destination cache\n",
    "#     dst = DynamicCache()\n",
    "\n",
    "#     for layer in range(len(cache)):\n",
    "#         K = cache.layers[layer].keys\n",
    "#         V = cache.layers[layer].values\n",
    "#         assert K is not None and V is not None\n",
    "\n",
    "#         _, H, S, D = K.shape\n",
    "\n",
    "#         K_new = K.new_zeros((1, H, S-2, D))\n",
    "#         V_new = V.new_zeros((1, H, S-2, D))\n",
    "\n",
    "#         K_new[0, :, :, :] = K[b, :, 2: :]\n",
    "#         V_new[0, :, :, :] = V[b, :, 2:, :]\n",
    "\n",
    "#         dst.update(K_new, V_new, layer)\n",
    "\n",
    "#     return dst\n",
    "\n",
    "# i = 2\n",
    "# cache = get_layer(cache, i)\n",
    "# tokens = [tokens[i]]\n",
    "\n",
    "# print_cache(cache, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38b2728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[66836], [8325], [25], [374]]\n",
      "tensor([ 5, 11,  6,  4], device='mps:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:00<00:03,  4.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '’s', '!', '!']\n",
      "['!', ' new', '!', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:00<00:03,  5.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', ' machine', '!', '!']\n",
      "['!', ' learning', '!', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [00:01<00:02,  5.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', ' framework', '!', '!']\n",
      "['!', ',', '!', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [00:01<00:02,  5.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', ' which', '!', '!']\n",
      "['!', ' is', '!', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [00:01<00:01,  5.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', ' designed', '!', '!']\n",
      "['!', ' to', '!', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [00:02<00:01,  5.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', ' make', '!', '!']\n",
      "['!', ' it', '!', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [00:02<00:01,  5.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', ' easier', '!', '!']\n",
      "['!', ' for', '!', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [00:02<00:00,  5.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', ' developers', '!', '!']\n",
      "['!', ' to', '!', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [00:03<00:00,  5.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', ' build', '!', '!']\n",
      "['!', ' machine', '!', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:03<00:00,  5.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', ' learning', '!', '!']\n",
      "['!', ' models', '!', '!']\n",
      "<|begin_of_text|>Explanation of speculative decoding speculative!!!!!!!!!!!!!!!!!!!\n",
      "<|begin_of_text|>This is a terse haiku about Apple MLX Apple’s new machine learning framework, which is designed to make it easier for developers to build machine learning\n",
      "<|begin_of_text|>def bubble_sort(x::!!!!!!!!!!!!!!!!!!!\n",
      "<|begin_of_text|>Why is the is!!!!!!!!!!!!!!!!!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "suffix_text = [\n",
    "    ' speculative',\n",
    "    ' Apple',\n",
    "    ':',\n",
    "    ' is',\n",
    "]\n",
    "generated = [tokenizer.encode(x)[1:] for x in suffix_text]\n",
    "print(generated)\n",
    "\n",
    "lengths = torch.tensor([len(x) for x in tokens], dtype=torch.long, device=model.device)\n",
    "print(lengths)\n",
    "\n",
    "for _ in tqdm(range(20)):\n",
    "    tokens: list[list[int]] = [x + y for x, y in zip(tokens, generated)]\n",
    "    # print(tokens)\n",
    "\n",
    "    generated, lengths = generate_step(model, cache, generated, lengths)\n",
    "    print([tokenizer.decode(x) for x in generated])\n",
    "\n",
    "for i in range(4):\n",
    "    print(tokenizer.decode(tokens[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a585ea50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[66836], [8325], [25], [374]]\n",
      "tensor([25, 31, 26, 24], device='mps:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:00<00:03,  5.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', ' ML', '!', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:00<00:03,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', 'X', '!', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:00<00:03,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', ' Apple', '!', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:00<00:02,  5.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '’s', '!', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [00:00<00:02,  5.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', ' new', '!', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [00:01<00:02,  5.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', ' machine', '!', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [00:01<00:02,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', ' learning', '!', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [00:01<00:02,  5.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', ' framework', '!', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [00:01<00:02,  5.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', ',', '!', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [00:01<00:01,  5.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', ' which', '!', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [00:01<00:01,  5.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', ' is', '!', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [00:02<00:01,  5.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', ' designed', '!', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [00:02<00:01,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', ' to', '!', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [00:02<00:01,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', ' make', '!', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [00:02<00:00,  5.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', ' it', '!', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [00:02<00:00,  5.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', ' easier', '!', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [00:03<00:00,  5.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', ' for', '!', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [00:03<00:00,  5.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', ' developers', '!', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [00:03<00:00,  5.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', ' to', '!', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:03<00:00,  5.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', ' build', '!', '!']\n",
      "<|begin_of_text|>Explanation of speculative decoding speculative!!!!!!!!!!!!!!!!!!! speculative!!!!!!!!!!!!!!!!!!!\n",
      "<|begin_of_text|>This is a terse haiku about Apple MLX Apple’s new machine learning framework, which is designed to make it easier for developers to build machine learning Apple MLX Apple’s new machine learning framework, which is designed to make it easier for developers to\n",
      "<|begin_of_text|>def bubble_sort(x::!!!!!!!!!!!!!!!!!!!:!!!!!!!!!!!!!!!!!!!\n",
      "<|begin_of_text|>Why is the is!!!!!!!!!!!!!!!!!!! is!!!!!!!!!!!!!!!!!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "suffix_text = [\n",
    "    ' speculative',\n",
    "    ' Apple',\n",
    "    ':',\n",
    "    ' is',\n",
    "]\n",
    "# generated = [[x] for x in tokenizer.encode(suffix_text)[1:]] * 4\n",
    "generated = [tokenizer.encode(x)[1:] for x in suffix_text]\n",
    "print(generated)\n",
    "\n",
    "lengths = torch.tensor([len(x) for x in tokens], dtype=torch.long, device=model.device)\n",
    "print(lengths)\n",
    "\n",
    "for _ in tqdm(range(20)):\n",
    "    tokens: list[list[int]] = [x + y for x, y in zip(tokens, generated)]\n",
    "    # print(tokens)\n",
    "\n",
    "    generated, lengths = generate_step(model, cache, generated, lengths)\n",
    "    print([tokenizer.decode(x) for x in generated])\n",
    "\n",
    "for i in range(4):\n",
    "    print(tokenizer.decode(tokens[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b10fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([45, 51, 46, 44], device='mps:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149aeba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suffix_text = '.'\n",
    "# generated = [[x] for x in tokenizer.encode(suffix_text)[1:]] * 3\n",
    "\n",
    "# lengths = torch.LongTensor([len(x) for x in tokens], device=model.device)\n",
    "\n",
    "# for _ in tqdm(range(20)):\n",
    "    \n",
    "\n",
    "#     generated, lengths = generate_step(model, cache, tokens, lengths)\n",
    "#     tokens: list[list[int]] = [x + y for x, y in zip(full_tokens, tokens)]\n",
    "\n",
    "# for i in range(3):\n",
    "#     print(tokenizer.decode(full_tokens[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e7129e",
   "metadata": {},
   "source": [
    "# Bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfda1995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rollback_dynamic_per_row(cache: DynamicCache, r: torch.LongTensor):\n",
    "#     \"\"\"\n",
    "#     Roll back r[i] tokens for each batch row i in a DynamicCache.\n",
    "#     Returns a *new* DynamicCache with time dim equal to max(L_i - r_i).\n",
    "#     \"\"\"\n",
    "#     assert cache.layers[0].keys is not None\n",
    "#     B = cache.layers[0].keys.shape[0]\n",
    "#     device = cache.layers[0].keys.device\n",
    "#     uniq = torch.unique(r)\n",
    "\n",
    "#     # Build empty destination (we'll fill layer by layer)\n",
    "#     dst = DynamicCache()\n",
    "#     for layer in range(len(cache)):\n",
    "#         K = cache.layers[layer].keys\n",
    "#         V = cache.layers[layer].keys\n",
    "#         assert K is not None\n",
    "#         assert V is not None\n",
    "\n",
    "#         B, H, S_old, D = K.shape\n",
    "#         # Compute new per-row lengths after rollback\n",
    "#         L_after = torch.full((B,), S_old, dtype=torch.long, device=device) - r\n",
    "#         S_new = int(L_after.max().item())\n",
    "\n",
    "#         K_new = K.new_zeros(B, H, S_new, D)\n",
    "#         V_new = V.new_zeros(B, H, S_new, D)\n",
    "\n",
    "#         # For each rollback bucket, crop and scatter back\n",
    "#         for rv in uniq.tolist():\n",
    "#             idx = (r == rv).nonzero(as_tuple=False).squeeze(-1)\n",
    "#             if idx.numel() == 0: \n",
    "#                 continue\n",
    "#             # Select sub-batch, crop rv tokens from the right\n",
    "#             K_sub = K.index_select(0, idx)\n",
    "#             V_sub = V.index_select(0, idx)\n",
    "\n",
    "#             # physical crop for this bucket\n",
    "#             S_keep = S_old - rv\n",
    "#             K_sub = K_sub[..., :S_keep, :]\n",
    "#             V_sub = V_sub[..., :S_keep, :]\n",
    "\n",
    "#             # place back into dst; zero-filling beyond S_keep keeps them \"rolled back\"\n",
    "#             K_new.index_copy_(0, idx, torch.nn.functional.pad(K_sub, (0,0,0,0,0, S_new - S_keep)))\n",
    "#             V_new.index_copy_(0, idx, torch.nn.functional.pad(V_sub, (0,0,0,0,0, S_new - S_keep)))\n",
    "\n",
    "#         dst.update(K_new, V_new, layer)\n",
    "\n",
    "#     return dst\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
